In this section we collect some recommendations for constructing convolutional networks as well as efficiently training deep neural networks. These are intended to be specific to this project but most will also be useful in similar projects.
% Some of these details are taken care by the software(either as a defualt or optional feature) and some are qyuite new and need to be taken care manually.
%technical concepts
% the reasoning behind is explained on the references sources. 

%mamogrpahy images are sufficiently different from images used in other object recognition challenges for example labeling may not be as sharp, they are not color mages, the object to recognize is quite small compared to the image, ammograms come in different sizes and the usable part is rectangular (higher than wider), mammogrpahuics image are sometimes required to identify the object and localize it which is not a requirement on other data. and thus some of these advice may prove counterproductive. We will start with the most weel tested features, diagnose the model and add features which willl hopefully produce beter vresults and work this way until obtaining the best model which will be trained and tested for a final time.

\paragraph{Image preprocessing} Some standard processing for images.
\begin{itemize}
	\item Images are cropped to contain only the relevant parts of the image, denoised, enhanced and optionally downsampled to maintain the input size manageable.

	\item Each image feature (the raw pixels) is zero centered by substracting its mean across all training images. Normalization scales the already zero-centered features to range from $[-1 \dots 1]$ by dividing them by its standard deviations. Feature normalization is not striclty neccesary but still customary~\cite{Karpathy2015}.

	\item The test data should not be used to calculate any statistic used to preprocess the training data. Furthermore, these same statistics (calculated from the training data) should later be used when normalizing the test data~\cite{Karpathy2015}.
\end{itemize}

\paragraph{Convolutional network architecture}
We offer some guidelines for designing convolutional network architectures and some standard values for various hyperparameters.

\begin{itemize}
	\item It is always better to select a complex network architecture which is flexible enough to model the data and manage overfitting with regularization rather than an architecture which is not powerful enough to model the data~\cite{Ng2014, Krizhevsky2012}. 

	\item Although, theoretically, neural networks with a single hidden layer are universal approximators provided they have enough units ($\mathcal{O}(2^n)$ where $n$ is the size of the input), in practice, deeper architectures produce better results using less units overall. This insight holds for convolutional networks~\cite{Bengio2014}.

	\item As a rule of thumb for big data sets, use 8-20 layers (not counting pooling or ReLU layers). For small data sets, use less layers or transfer learning. ``You should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.''~\cite{Karpathy2015}

	\item Use the number of parameters rather than the number of layers or units as a measure of the architecture's complexity.

	\item Use 2-3 \texttt{CONV -> RELU} pairs before pooling (N above)~\cite{Karpathy2015}. Pooling is a destructive operation and having two convolutional layers together allows them to pick up more complex features.

	\item Use 1-5 \texttt{[CONV -> RELU]+ -> POOL} blocks (M above). This number depends on the complexity of the features expected in the data and the computational resources available. In a way, this regulates how much representational power will the architecture have. It also decides how much the volume is subsampled.

	\item Use less than 3 \texttt{FC -> RELU} pairs before the output layer (K above)~\cite{Karpathy2015}. When the volume arrives to the fully connected layers it has shrinken enough and using more fully connected layers risks overfitting.

	\item The number of feature maps per convolutional layer is set according to the expected number of features. This is similar to the number of units in a regular neural network. A common pattern is to start with a small amount of feature maps and increase them layer by layer~\cite{Simonyan2014}. %The reasoning is that at higher layers there are more complex features to learn and moreover as the feature maps become smaller (via pooling) it is computationally feasible to have more of them.

	\item The number of feature maps per fully connected layer, or equivalently the number of units per fully connected layer, decreases from the number of units in the last convolutional layer (the number of units in each feature map times the number of feature maps) to the number of classes. For instance, having a convolutional network with two fully connected layers and 10 possible classes if the last convolutional layer produces a volume of size $8 \times 8 \times 512$ (8192 units), the first fully connected layer could have size $1 \times 1 \times 2048$ and the second (output) layer $1\times 1\times 10$.

	\item Use $3\times 3$ filters with stride 1 and zero-padding 1 or $5 \times 5$ filters with stride 1 and zero-padding 2. This preserves the spatial dimensions of the volume and works better in practice~\cite{Springenberg2014}. When training on big images, the first convolutional layer uses bigger filters~\cite{Karpathy2015}.

%Cite Lin, simplicity
	
	\item Use $2\times2$ pooling with stride 2. Both this pooling and the overlapping version presented in Section~\ref{subsec:ConvNets} produce similar results. This pooling divides the spatial dimensions of the volume by half.

	\item Use square input images (width = height) with dimensions divisible by 2. The dimensions should be divisible by 2 at least as many times as the number of pooling layers in the network.

	\item Convert fully connected layers into convolutional layers.
\end{itemize}


\paragraph{Hyperparameter search}
We deal here with choosing hyperparameters other than those of the network architecture.
\begin{itemize}
\item there are many hyperparameters: ...,. furthermore there are many related to the convlutional network architecture: ...

\item Important Hyperparameters: Learning rate very important. Others will report only a slight improvement and could be set manually.Hyperparameter fitting: preprocessing, learning rate, lambda. size of the neural network (just big). Karpathy

\item Use one validation with a big validation set (Bengio).

\item hyperparameter search random Bengio, 2012 karpathy.

\item Search in log scale. se exponential scales: 0.1, 0.01, .001

\item momentum = [0.5, 0.9, 0.95, 0.99] or 0.9

\item for learning rate also plot error/loss vs epochs, choos ethe one that converges fast or gives the better learning rate.
\end{itemize}
% Hyperparameters: preprocessing(4), small vs big image, regularization param, learning parameter, mu(momentum) 0.9 , p(dropout) 0.5, filter size, learning rate schedule,...


\begin{comment}




\paragraph{Dealing with overfitting}
t is normally recommended to have a flexible model and convtrol for ovefittting. this can be done by various means. plus regularization.


Dat augmentation:
in order to generate additional examples from tehe data
 flipping over x, training the network on smaller pathces selected from the original image (a similar approach is used in testing to made predictions), selecting only smaller patches of the original image, rotations, and adding random noise to the image (jittering the colors)(). Most of this do not need to be stored explicitly but can be generated during training. 
Data augmenattion rotations and flipping over axis (Krhizhevsky, as in GalaxyZoo and Lo et al.)

Dropout(hinton et al., 2012): averaging over many mehtods.
Regularization: l2-norm normally with Dropout(Srivastava et al, 2014), works like that, doe sthat and that. p=0.5, scale the activations. Only subsample a neural network to copute the output and make an update. Inverted dropout recomended. Hyperpaameters: p, lambda. 

\textbf{Training}:

Randomize trainig set before training

Sensible parameter initialization is very important: For ReLu each weight vector w = np.random.randn(n) * sqrt(2.0/n) (He et al, 2015). 0 bias initialization or small random numbers do not really matter.

Use minibatch stochastic gradient descent. No point on using the exact gradient because you ar enot going to advance much and it may not even be the right direction.Minibatch gradient descent. 256 examles.>100 Stochastc gradient descent is extreme with oine example.

Early stopping. free lunch, #of iterations per training should not be thought as ahyperparameter. You can just rusn until it strts overfitting or it doesn{t improve anymore and preserve the best fitted Ã¡rameters. SO stop the execution every #of steps (multiple of validation set size) to see what{s the training error and how well will it do. Maybe juts set max number of iterations. important print error to see what's ging one. Use heuristic as stopping criteria, for example if I did x #iter to get to my best parameters (say 100) and in another #iter I have not improved it, then  it probably converged (so it will stop in 200).

Learning: sgd+momentum, with nag, Decreasing/decaying learning rate schedule: slowly(divided by two every number of steps)(do i need this?, maybe not).

l2 regularization.

\textbf{Checks}:
loss at chance performance
Overfit a tiny subset of data(cost=0). If i can't is not worthede going to a bigger dataset.
Gradient check, loss

\textbf{Unbalanced data}:
Change the threshold for unbalanced classes. Cross validate it to see which one is better, never use the test set. Explain why using one metric and not the other.
%Make sure I said that data replication (rotations and such) is needed because e don't have enough examples of a simple class, not because we are trying to balance the classes. Data replication is different.

\paragraph{Other things to try}
For transfer learning, given that a network trained in the ImageNet is very diffferent to the images we are going to test them on and our dtaaset is small/large, we should train a linear clasifier from the activations somewhere inside of the network (where the features obtained are probably more general)/ we could fine tune only the upper layers of the network. Plus when finetuning the learning rate is normally smaller that wehne training a network from scratch. \item Transfer Learning may not work cause the dataset is quite different.
\item Have a score for every space or apply the convolutional network to different parts of the input (this needs way too many parameters).

\item all convolutional Due to the aggressive reduction in the size of the representation (which is helpful only for smaller datasets to control overfitting), the trend in the literature is towards discarding the pooling layer in modern ConvNets.

\paragraph{Software} 
\item caffe
\item theano: Implments recommendations from Bengio
\item torch 
\item deeplearning4j
\end{comment}

%Deep Learning: (https://www.youtube.com/watch?v=JuimBuvEWBg)
%Bengio2013(techreport): Practical reccomendations for gradient based training of deep architecthure
%Activation function: although the sigmoid function is historically used it has fal out of favor... (http://cs231n.github.io/neural-networks-1/) because in the end of the outputs it is plabne and its graident \emph{vanish} (i.e., become 0), thus special care is needed when initializing it to otherwise it will not learn. Second drawback, it is not zero-centered (not so important, the first one is). Tanh is always preferred to the sigmoid non-linearity. ReLUs for the win (can die, need good learning rate)., maxouts(put formula, double the number of weights per neuron).

% Train a simple convolutional
% Train one with bells and whistles(choosen by heart)
% Improve on the two, how to deal with the small dataset (how small it is)
% analogical representations for free, htm for word representation. Representation Learning, 
% Idea: Stop the linear recombination and impose some sense into it, if every neuron learns a feature then to learn a face wyou will be summing ears and eyes and things like that but a lot of info is lost in the way, convolutional networks solve the problem for spatial info, but is there more, which is next to each other and how many of them they are. Neurons don't just take a weighted sum butwhat do they do?.
% Idea: what about only applying the pool after some epochs, because the pooling ewill select and encourage the first feature it founds on the 2by2 space, thus disencouraging exploration (exploration meaning filters for other things). I.e., as soon as the filter identifies some feature in the data the pooling will try to make this filter better for that specific feature but if there was a filter for another feature that is better it will not get learn. 
