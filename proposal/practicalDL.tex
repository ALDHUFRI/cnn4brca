In this section we collect some recommendations for constructing convolutional networks as well as efficiently training deep neural networks. These are intended to be specific to this project but most will also be useful in similar projects.
% Some of these details are taken care by the software(either as a defualt or optional feature) and some are qyuite new and need to be taken care manually. 

%mamogrpahy images are sufficiently different from images used in other object recognition challenges for example labeling may not be as sharp, they are not color mages, the object to recognize is quite small compared to the image, ammograms come in different sizes and the usable part is rectangular (higher than wider), mammogrpahuics image are sometimes required to identify the object and localize it which is not a requirement on other data. and thus some of these advice may prove counterproductive. We will start with the most weel tested features, diagnose the model and add features which willl hopefully produce beter vresults and work this way until obtaining the best model which will be trained and tested for a final time.

\paragraph{Image preprocessing} Some standard processing for images.
\begin{itemize}
	\item Images are cropped to contain only the relevant parts of the image, denoised, enhanced and optionally downsampled to maintain the input size manageable.

	\item Each image feature (the raw pixels) is zero centered by substracting its mean across all training images. Normalization scales the already zero-centered features to range from $[-1 \dots 1]$ by dividing them by its standard deviations. Feature normalization is not striclty neccesary but still customary~\cite{Karpathy2015}.

	\item The test data should not be used to calculate any statistic used to preprocess the training data. Furthermore, these same statistics (calculated from the training data) should later be used when normalizing the test data~\cite{Karpathy2015}.
\end{itemize}

\paragraph{Convolutional network architecture}
We offer some guidelines for designing convolutional network architectures and some standard values for various hyperparameters.

\begin{itemize}
	\item It is always better to select a complex network architecture which is flexible enough to model the data and manage overfitting with regularization rather than an architecture which is not powerful enough to model the data~\cite{Ng2014, Krizhevsky2012}. 

	\item Although, theoretically, neural networks with a single hidden layer are universal approximators provided they have enough units ($\mathcal{O}(2^n)$ where $n$ is the size of the input), in practice, deeper architectures produce better results using less units overall. This insight holds for convolutional networks~\cite{Bengio2014}.

	\item As a rule of thumb for big data sets, use 8-20 layers (not counting pooling or ReLU layers). For small data sets, use less layers or transfer learning. ``You should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.''~\cite{Karpathy2015}

	\item Use the number of parameters rather than the number of layers or units as a measure of the architecture's complexity.

	\item Use 2-3 \texttt{CONV -> RELU} pairs before pooling (N above)~\cite{Karpathy2015}. Pooling is a destructive operation and having two convolutional layers together allows them to pick up more complex features.

	\item Use 1-5 \texttt{[CONV -> RELU]+ -> POOL} blocks (M above). This number depends on the complexity of the features expected in the data and the computational resources available. In a way, this regulates how much representational power will the architecture have. It also decides how much the volume is subsampled.

	\item Use less than 3 \texttt{FC -> RELU} pairs before the output layer (K above)~\cite{Karpathy2015}. When the volume arrives to the fully connected layers it has shrinken enough and using more fully connected layers risks overfitting.

	\item The number of feature maps per convolutional layer is set according to the expected number of features. This is similar to the number of units in a regular neural network. A common pattern is to start with a small amount of feature maps and increase them layer by layer~\cite{Simonyan2014}. %The reasoning is that at higher layers there are more complex features to learn and moreover as the feature maps become smaller (via pooling) it is computationally feasible to have more of them.

	\item The number of feature maps per fully connected layer, or equivalently the number of units per fully connected layer, decreases from the number of units in the last convolutional layer (the number of units in each feature map times the number of feature maps) to the number of classes. For instance, having a convolutional network with two fully connected layers and 10 possible classes if the last convolutional layer produces a volume of size $8 \times 8 \times 512$ (8192 units), the first fully connected layer could have size $1 \times 1 \times 2048$ and the second (output) layer $1\times 1\times 10$.

	\item Use $3\times 3$ filters with stride 1 and zero-padding 1 or $5 \times 5$ filters with stride 1 and zero-padding 2. This preserves the spatial dimensions of the volume and works better in practice~\cite{Springenberg2014}. When training on big images, the first convolutional layer uses bigger filters~\cite{Karpathy2015}.
	
	\item Use $2\times2$ pooling with stride 2. Both this pooling and the overlapping version presented in Section~\ref{subsec:ConvNets} produce similar results. This pooling divides the spatial dimensions of the volume by half.

	\item Use square input images (width = height) with dimensions divisible by 2. The dimensions should be divisible by 2 at least as many times as the number of pooling layers in the network.

	\item Convert fully connected layers into convolutional layers.
\end{itemize}



\paragraph{Hyperparameter search}
We deal here with choosing hyperparameters other than those of the network architecture.

\begin{itemize}

	\item Use a single sufficiently large validation set (20-30\%) rather than cross validation~\cite{Bengio2014}. For small data sets, cross validation can give better estimates and is preferred~\cite{Ng2014}.

	\item Use random search rather than grid search. Random search draws each parameter from a value distribution rather than a set of predefined values.~\cite{Bergstra2012}

	\item Train each parameter combination for 1-2 epochs to narrow the search space. Later, train for more epochs on the refined ranges. Full convergence is not needed to make a decision on the hyperparameters~\cite{Karpathy2015}.

	%\item If you need to run the hyperparameter search more than once, choose a different validation set. :: I don't remember the reasoning. I think it was so that 

	\item Hyperparameters related to the convolutional architecture, e.g., number of layers, number of feature maps, filter sizes, etc., are set manually (as explained above) rather than using a validation set.
% M,N,K, filter size, stride and padding in each convolutional layer, feature maps and units in CONV/FC layers, pooling

	\item There are several hyperparameters to set: initial learning rate $\alpha$, learning rate decay schedule, regularization strength $\lambda$, momentum $\mu$, dropout probability $p$, mini-batch size and type of image preprocessing.

	\item Theoretically we could fit all the hyperparameters using a validation set but in practice it is computationally unfeasible and could result in overfitting the hyperparameters to the validation data~\cite{Cawley2010}.

	\item Set $\alpha$, $\lambda$ and optionally the type of preprocessing using a validation set. Other hyperparameters are set to a sensible default.
%The learning rate schedule and training epochs are set using heuristics. 


	\item The learning rate $\alpha$ is ``the single most important hyperparameter and one should always make sure that it has been tuned''~\cite{Bengio2012}. It ranges from $10^{-6}$ to $10$. Use a log scale to draw new values ($\alpha = 10^{unif(-6, 1)}$ where $unif(a,b)$ is the continous uniform distribution)~\cite{Karpathy2015}.
%0.01

	\item The regularization strength $\lambda$ is usually data (and loss function) dependant. It ranges from $10^{-3}$ to $10^4$. Search in log scale ($\lambda = 10^{unif(-3, 4)}$).
%1

	\item If the best values for a hyperparameter are found in the limit of the range, explore further.~\cite{Bengio2012}.

	\item Use standard image enhancements. If there are no standard methods, use the validation set to choose from the options.

	\item Halve the learning rate every time the validation error stops improving. To obtain a fixed number of epochs, train the network (with the obtained hyperparameters) and observe when the validation error stops decreasing~\cite{Krizhevsky2012}.

	\item Use $\mu=0.9$. When using a validation set try values in \{0.5, 0.9, 0.95, 0.99\}~\cite{Karpathy2015}.

	\item Use dropout probability $p = 0.5$~\cite{Karpathy2015}. Lower values mean less dropout.

	\item Use mini-batch size of 32 or 64. A larger batch size requires more training time. It affects training time more than test performance~\cite{Bengio2012}.

\end{itemize}



\paragraph{Training}
Some general tips for efficiently training convolutional networks with million of parameters and very big data sets. Using these algorithms for small networks may be somewhat excessive but it will not hurt the performance.

\begin{itemize}
	\item Randomize the order of the trainig examples before training. As we are using an stochastic estimator of the gradient this ensures the examples in each batch are sampled independently~\cite{Bengio2012}.

	\item Weight initialization is very important for a proper convergence of the network. The current recommendation for ReLU units is to initialize each weight as a value drawn from a gaussian distribution $\mathcal{N}(\mu = 0, \sigma = \sqrt{2/n_{in}})$ where $n_{in}$ is the fan-in of the unit, i.e, the number of inputs to this unit. Specifically, each filter weight could be initialized as \texttt{w = randn()*sqrt(2/nIn)} where \texttt{randn()} returns a value drawn from a standard normal distribution and \texttt{nIn} is the number of connections to this filter (9 for a $3\times 3$ filter, for example). Weights for units in the fully connected layer follow the same formula. Biases can be initialized likewise or to zero~\cite{He2015}.

	\item Use mini-batches to compute the gradient. Using the entire training set to compute the gradient of the loss function takes a big amount of computation and points to the steepest descent direction locally but may not be the right direction if the update step is large. Using mini-batches allows us to make more updates, more frequently which result in faster convergence and better test results~\cite{Bengio2012}.

	\item Use Nesterov's Accelerated Gradient (NAG) to update the weights. It is a modified version of Stochastic Gradient Descent with Momentum (SGD+Momentum) which has shown to work slightly better for recurrent networks. SGD+Momentum is also a viable option~\cite{Karpathy2015}.
% May need to validate the momentum if using NAG 

	\item Store the network parameters regularly during training. Once per epoch should be enough but it depends on the number of parameters and size of the data. This allows you to come back to different versions of the network and select the one with the best overall validation/test error or one with some special characteristics~\cite{Bengio2014}.

	\item Stop the training process when the validation or test error has not improved since the last learning rate reduction. At this point gradient descent may not have converged but the validation error has and will start to increase~\cite{Bengio2012}.

	\item Use the validation or test error to select the best parameters for the network from those stored~\cite{Bengio2014}. 

	\item If you use the test set to refine a model, shuffle the entire data set and choose a diferent training and test set for the new model. Otherwise, you run the risk of overfitting to the test set~\cite{Ng2014}.
\end{itemize}


\paragraph{Sanity checks}
Some simple checks we can run to make sure everything is alright. 
%Say that them depend on wether you did it manually.
\begin{itemize}
	\item Make sure that with random initializations it gives yuloss at chance performance. Loss should go up-increase with more regularization. No regulariaiton at first.

	\item If you implemented back propagation manually or have reasons to believe that it is not working properly, doing a numerical gradient check is a very easy way to confirm your intuitions~\cite{Karpathy2015}.

		\item Overfit a tiny subset of data(cost=0). If i can't is not worthede going to a bigger dataset. If it cannot, then the modelis too simple.

	\item Training Loss should always decrease or only slightly increase. Wehn it foundas a plateau, reducing the learning rate as explained above could make it decrease more. 

\end{itemize}


\begin{comment}




\paragraph{Dealing with overfitting}
t is normally recommended to have a flexible model and convtrol for ovefittting. this can be done by various means. plus regularization.


Dat augmentation:
in order to generate additional examples from tehe data
 flipping over x, training the network on smaller pathces selected from the original image (a similar approach is used in testing to made predictions), selecting only smaller patches of the original image, rotations, and adding random noise to the image (jittering the colors)(). Most of this do not need to be stored explicitly but can be generated during training. 
Data augmenattion rotations and flipping over axis (Krhizhevsky, as in GalaxyZoo and Lo et al.)

Use dropout to complement regularization, dropout does that, see ... for an explanation.
Dropout(hinton et al., 2012): averaging over many mehtods.
Regularization: l2-norm normally with Dropout(Srivastava et al, 2014), works like that, doe sthat and that. scale the activations. Only subsample a neural network to copute the output and make an update. Inverted dropout recomended. Hyperpaameters: p, lambda. 

\textbf{Unbalanced data}:
Change the threshold for unbalanced classes. Cross validate it to see which one is better, never use the test set. Explain why using one metric and not the other.
%Make sure I said that data replication (rotations and such) is needed because e don't have enough examples of a simple class, not because we are trying to balance the classes. Data replication is different.

\paragraph{Transfer learning and all convolutional}
For transfer learning, given that a network trained in the ImageNet is very diffferent to the images we are going to test them on and our dtaaset is small/large, we should train a linear clasifier from the activations somewhere inside of the network (where the features obtained are probably more general)/ we could fine tune only the upper layers of the network. Plus when finetuning the learning rate is normally smaller that wehne training a network from scratch. 
\item Transfer Learning may not work cause the dataset is quite different.
Have a score for every space.

\item all convolutional. trying all convolutional, what size what stride. replacing pooling layers (which of the three versions in the paper worked) "Due to the aggressive reduction in the size of the representation (which is helpful only for smaller datasets to control overfitting), the trend in the literature is towards discarding the pooling layer in modern ConvNets." For small data sets pooling also works as a regularizer (because it greatly reduces the number of learnable parameters) so having all convolutional layers may not be a good thing. 

\item unblanaced, use f2.
\item When dealing with unbalanced data set the threshold(cite inbalanced data)
 After transforming the score vector to a probability distribution we should decide what the treshold  is 


\paragraph{Software} 
\item caffe
\item theano: Implments recommendations from Bengio
\item torch 
\item deeplearning4j

\bigskip

%mamogrpahy images are sufficiently different from images used in other object recognition challenges for example labeling may not be as sharp, they are not color mages, the object to recognize is quite small compared to the image, ammograms come in different sizes and the usable part is rectangular (higher than wider), mammogrpahuics image are sometimes required to identify the object and localize it which is not a requirement on other data. and thus some of these advice may prove counterproductive. We will start with the most weel tested features, diagnose the model and add features which willl hopefully produce beter vresults and work this way until obtaining the best model which will be trained and tested for a final time
\end{comment}



%  Maybe use this order: training, dealing with overfitting, hyperparameters and everything else.

%Deep Learning: (https://www.youtube.com/watch?v=JuimBuvEWBg)
%Bengio2013(techreport): Practical reccomendations for gradient based training of deep architecthure
%Activation function: although the sigmoid function is historically used it has fal out of favor... (http://cs231n.github.io/neural-networks-1/) because in the end of the outputs it is plabne and its graident \emph{vanish} (i.e., become 0), thus special care is needed when initializing it to otherwise it will not learn. Second drawback, it is not zero-centered (not so important, the first one is). Tanh is always preferred to the sigmoid non-linearity. ReLUs for the win (can die, need good learning rate)., maxouts(put formula, double the number of weights per neuron).

% Train a simple convolutional
% Train one with bells and whistles(choosen by heart)
% Improve on the two, how to deal with the small dataset (how small it is)
% analogical representations for free, htm for word representation. Representation Learning,
% IN methodology write a section called operationalization were i explain how the problem is reduced to a clsssification task(what are the labels and how the images are cropped and what are the performance metrics)
