\chapter{Results}
\label{ch:Results}
We show the results of the different experiments and discuss their implications.

\section{Hyperparameter Search}
We list the selected learning rate and regularization parameter for the five folds of each experiment (Tab~\ref{tab:HSResults}). Although hyperparameters were selected independently for every fold, in the same experiment they converge to similar values.

\begin{table}[h]
	\centering
	\setlength\tabcolsep{3.5pt}
	\begin{tabular}{l*{10}{c}}
		\hline
		 & \multicolumn{2}{c}{\textbf{Fold 1}} & \multicolumn{2}{c}{\textbf{Fold 2}} &\multicolumn{2}{c}{\textbf{Fold 3}} &\multicolumn{2}{c}{\textbf{Fold 4}} &\multicolumn{2}{c}{\textbf{Fold 5}}\\
		& \textbf{$\alpha$} & \textbf{$\lambda$} & \textbf{$\alpha$} & \textbf{$\lambda$} & \textbf{$\alpha$} & \textbf{$\lambda$} & \textbf{$\alpha$} & \textbf{$\lambda$} & \textbf{$\alpha$} & \textbf{$\lambda$}\\
		\hline 
		Exp. 1.1	&\sn{2}{-4}	&\sn{3}{-2}	&\sn{2}{-4}	&\sn{4}{-2}	&\sn{9}{-5}	&\sn{5}{-3}	&\sn{6}{-4}	&\sn{8}{-3}	&\sn{6}{-4}	&\sn{1}{-2}\\
		Exp. 1.2	&\sn{4}{-4}	&\sn{9}{-2}	&\sn{4}{-4}	&\sn{5}{-2}	&\sn{4}{-4}	&\sn{7}{-3}	&\sn{3}{-4}	&\sn{5}{-3}	&\sn{6}{-4}	&\sn{3}{-2}\\
		Exp. 1.3	&\sn{4}{-4}	&\sn{3}{-2}	&\sn{2}{-4}	&\sn{4}{-2}	&\sn{7}{-5}	&\sn{6}{-3}	&\sn{4}{-4}	&\sn{8}{-2}	&\sn{8}{-4}	&\sn{3}{-2}\\
		Exp. 2		&\sn{1}{-5}	&\sn{4}{-4}	&\sn{5}{-5}	&\sn{4}{-3}	&\sn{5}{-5}	&\sn{1}{-3}	&\sn{1}{-5}	&\sn{2}{-4}	&\sn{5}{-5}	&\sn{2}{-4}\\
		Exp. 3		&\sn{1}{-4}	&\sn{4}{-4}	&\sn{7}{-5}	&\sn{5}{-4}	&\sn{1}{-4}	&\sn{7}{-4}	&\sn{7}{-5}	&\sn{2}{-4}	&\sn{8}{-5}	&\sn{9}{-5}\\
		\hline
	\end{tabular}
	\caption[Hyperparameter search results]{Selected learning rate and regularization parameter configurations.}
	\label{tab:HSResults}
\end{table}
% Discussion: Lambda is not as important as alpha. Highest alpha causes divergence, highest lambda causes convergence to a single point. In general, not so important.

\section{Experiments}
Models perform similarly while commiting less than four false positives per image with experiment 1.1 (in red), 1.3 (in green) and 2 (in blue) slightly outperforming the others (Fig.~\ref{fig:FROCResults}). After that mark, Experiment 1.1 and 1.3 converge faster to a sensitivity of 1 and experiment 2 follows them with a slightly slower convergence. Performance varies significantly among different folds of the same experiment as noticed by the spread of the background curves. Comparing experiment 1.1, 1.2 and 1.3, we also notice that for the same network architecture using a weighted loss function either with or without enhancing mammograms worsens the performance.
\begin{figure}
	\centering
		\includegraphics[width=\textwidth]{plots/FROC_curves.pdf}
	\caption[FROC curves]{FROC curves for every experiment and fold. Each experiment appears in a different color. The solid line is the average FROC curve per experiment over all folds. FROC curves ffor each fold are drawn in dashed lines; each fold has a unique marker.}
	\label{fig:FROCResults}
\end{figure}

We analyze the sensitivity at one false positive per image (Tab~\ref{tab:SensitivityResults}) to have a single quantitative evaluation of the predictions made by each model. We choose a low number of false positives to analyze the performance when models are commiting few errors. We notice that performance is similar among different models but that experiment 2 outperforms simpler models. It is also clear that performance is also dependent on the training and test set used shown by the different average performance in each fold.
%Discussion√ë too small test sets. 
\begin{table}[h]
	\centering
	\begin{tabular}{l*{6}{c}}
		\hline
		 & \textbf{Fold 1} & \textbf{Fold 2} & \textbf{Fold 3} &\textbf{Fold 4} &\textbf{Fold 5} & \textbf{Average} \\
		\hline 
		Experiment 1.1	&0.09		&\textbf{0.29}	&0.28	&0.29	&0.38	&0.26\\
		Experiment 1.2	&0.15		&0.28		&0.13		&0.26	&0.43	&0.25\\
		Experiment 1.3	&0.12		&0.25		&0.21			&0.24	&0.38	&0.24\\
		Experiment 2	&\textbf{0.21} &0.21	&\textbf{0.35}		&\textbf{0.30}	&0.41	&\textbf{0.29}\\
		Experiment 3	&0.18		&0.21		&0.22		&0.26	&\textbf{0.48}	&0.27\\
		Average			&0.15		&0.25		&0.23			&0.27	&0.42&\\
		\hline
	\end{tabular}
	\caption[Sensitivity at 1 FP/image for the final models]{Sensitivity at 1 FP per image. Best model in each column is shown in bold.}
	\label{tab:SensitivityResults}
\end{table}
% discussion: That there is a lot of variance depending on what fold you are in, this is mostly due to the examples in the test set than those in the training set as those in the training set
% discussion: that x model is usually better
% discussion: Effect on 1 of enhancement and weighted loss.
% discussion: that using a weighted loss usually help converged into a higher range (rather than all being very negative close to zero, now predictions ranged from 0-1)
% discussion: training with loss function is more stable, (ranges are not so low)

Finally, to get a sense of the qualitative value of our results, we examined the output produced by each network in some chosen examples (Appendix~\ref{app:examples}). Networks used for experiment 2 and 3 seem to outperform those from the first experiment, they perform well when the breast mass is easier to see in the mammogram and are less prone to signal chest muscle as a possible mass. The network from experiment 2 was specially impressive in signaling some of the more subtler lessions, while none of the other networks excelled at this task.

\section{Discussion}
We found that convolutional neural networks are a viable option for breast cancer lession segmentation. 

More research is needed to confirm some of the insights from this work. 

\begin{comment}
\section{Summary}
Results for Experiment 1 and Experiment 2 were both equally poor. Experiment 3 produced average results. We believe that it is because of this and this. this could be done... or whatever.
\end{comment}
