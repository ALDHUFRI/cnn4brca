\chapter{Results}
\label{ch:Results}
We show the results of the different experiments and discuss their implications.

\section{Experiment 1}
\subsection{Hyperparameter search}
After the first hyperparameter search, we found that the best $\alpha$ and $\lambda$ values appear in the bottom of the search range with good $\alpha$s ranging from 0.000001 to 0.0001 and $\lambda$s ranging from 0 to 0.15 (Fig.~\ref{fig:Hs1}).
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs1_alpha.png}
         \caption{IOU vs $\alpha$}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs1_lambda.png}
         \caption{IOU vs $\lambda$}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs1_pcolor.png}
         \caption{IOU (color) vs $\alpha$ (x) vs $\lambda$ (y)}
	\end{subfigure}
	\caption[First hyperparameter search for Experiment 1]{Results of the first search of hyperparameters. High IOU is better.}
	 \label{fig:Hs1}
\end{figure}

% Discussion: Divergenece and it can be also observed that lambda is not as important as alpha. Highest alpha causes divergence, highest lambda causes convergence to a single point.
After refining the search range we had a better idea of which combinations produced the best results (Fig.~\ref{fig:Hs2}).
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs2_alpha.png}
         \caption{IOU vs $\alpha$}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs2_lambda.png}
         \caption{IOU vs $\lambda$}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs2_trisurface.png}
         \caption{IOU (z) vs $\alpha$ (x) vs $\lambda$ (y)}
	\end{subfigure}
	\caption[Second hyperparameter search for Experiment 1]{Results of the second search of hyperparameters. High IOU is better.}
	 \label{fig:Hs2}
\end{figure}

With this results, we chose to train a network with $\alpha = 3 \times 10^{-6}$ and $\lambda = 5 \times 10^{-4}$.

\subsection{Evaluation}
After training the network, we used the validation set to select the best threshold: 0.86 probability, which produced the best IOU (0.35) in the validation set.

The test set results are summarized in Table~\ref{tab:Results1}
\begin{table}[h]
	\centering
	\begin{tabular}{cccccccc}
	\hline
	\textbf{IOU}	& \textbf{F1-score}	& \textbf{G-mean} &\textbf{Accuracy}	& \textbf{Sensitivity} & \textbf{Specificity} & \textbf{Precision} & \textbf{Recall}\\
	\hline
	- & - & - & - & - & - & - & -\\
	\hline
	\end{tabular}
	\caption[Results for Experiment 1]{Test results for the trained network (Sec~\ref{sec:Experiment1}).}
	\label{tab:Results1}
\end{table}

\subsection{Qualitative results}
We show the prediction and produced segmentation for some test examples.% in appendix A

%Images of original mammogram and label and predicitions (logits) and final segmentation


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Experiment 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment 2}
\subsection{Hyperparameter search}
Similar to experiment 1, the best $\alpha$s and $\lambda$s were found in the bottom of the search range (Fig.~\ref{fig:Hs4}), however, performance is more robust to small changes. We show results discarding networks with the ten highest $\alpha$s or ten highest $\lambda$s as bigger values caused training to diverge.
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs4_alpha.png}
         \caption{IOU vs $\alpha$}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs4_lambda.png}
         \caption{IOU vs $\lambda$}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs4_pcolor.png}
         \caption{IOU (color) vs $\alpha$ (x) vs $\lambda$ (y)}
	\end{subfigure}
	\caption[Hyperparameter search for Experiment 2]{Results of the hyperparameter search. High IOU is better.}
	 \label{fig:Hs4}
\end{figure}

With this results we selected $\alpha = 2 \times 10^{-5}$ and $\lambda = 2 \times 10^{-3}$ to train our network.

\subsection{Evaluation}
We obtained the best IOU in the validation set (0.36) with threshold probability 0.72; test set results are summarized below (Tab.~\ref{tab:Results2}).
\begin{table}[h]
	\centering
	\begin{tabular}{cccccccc}
	\hline
	\textbf{IOU}	& \textbf{F1-score}	& \textbf{G-mean} &\textbf{Accuracy}	& \textbf{Sensitivity} & \textbf{Specificity} & \textbf{Precision} & \textbf{Recall}\\
	\hline
	 - & - & - & - & - & - & - & -\\
	\hline
	\end{tabular}
	\caption[Results for Experiment 2]{Test results for the trained network (Sec~\ref{sec:Experiment2}).}
	\label{tab:Results2}
\end{table}

\subsection{Qualitative results}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EXPERIMENT 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment 3}
\subsection{Hyperparameter search}
As with previous experiments the best $\alpha$ and $\lambda$ values appear closer to the bottom of the search range (Fig.~\ref{fig:Hs5}). We chose to use $\alpha = 4 \times 10^{-5}$ and $\lambda = 4 \times 10^{-4}$ for our final network.
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs5_alpha.png}
         \caption{IOU vs $\alpha$}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs5_lambda.png}
         \caption{IOU vs $\lambda$}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
                \includegraphics[width=\textwidth]{plots/hs5_pcolor.png}
         \caption{IOU (color) vs $\alpha$ (x) vs $\lambda$ (y)}
	\end{subfigure}
	\caption[Hyperparameter search for Experiment 3]{Results of the hyperparameter search. High IOU is better.}
	 \label{fig:Hs5}
\end{figure}

\subsection{Evaluation}
\begin{comment}
After training, 0.6 probability threshold (logit 0.405) was selected using the validation set (IOU on validation set achieved 0.149). Test set results are summarized in Table~\ref{tab:Results1}
\begin{table}[h]
	\centering
	\begin{tabular}{cccccccc}
	\hline
	\textbf{IOU}	& \textbf{F1-score}	& \textbf{G-mean} &\textbf{Accuracy}	& \textbf{Sensitivity} & \textbf{Specificity} & \textbf{Precision} & \textbf{Recall}\\
	\hline
	0.089 & 0.121 & 0.219 & 0.974 & 0.185 & 0.979 & 0.121 & 0.185\\
	\hline
	\end{tabular}
	\caption[Results for Experiment 3]{Test results for the trained network (Sec~\ref{sec:Experiment3}).}
	\label{tab:Results3}
\end{table}

\subsection{Qualitative results}
\end{comment}


\section{Discussion}
We found that....

\begin{comment}
\section{Summary}
Results for Experiment 1 and Experiment 2 were both equally poor. Experiment 3 produced average results. We believe that it is because of this and this. this could be done... or whatever.
\end{comment}
