%We document the retrieval, enhancement, labelling and augmentation of [the|our] data set [used for the experiments].
% We document how we retrieve, enhance, label and augment our data set.
We document the retrieval, enhancement, labelling and augmentation of our data set.

\subsection{Database}
We use the Breast Cancer Digital Repository (BCDR-DM) database. In particular, we use the BDCR-D01 data set, which is composed of patients with at least one breast mass. We have curated it to delete patients with breast implants or other extraneous features ().

We select x digital mammograms from x patients. Theya are 8-bit images with 0.07mm spatial resolution whose size is ta, ta or ta, depending on the size of the placa used to obtain the pictures. Each lesion is segmented and its ... (labels), ..., ..., ... is supplied. Other patient data and image features are supplied but we do not make use of them.
% Files and how are they organized. Data available per case and per mammogram. How are boundaries written. formats, etc.

Digital mammograms have higher image quality and lack any marks, stamps or other scanning artifacts present in digitized film mammograms; this allows us to pick up better features and eases segmentation. Although the number of mammograms is small, training on overlapping patches and data augmentation, allows learning.

Labels were generated using the provided outlines and thresholding the background to zero. Masses (benign or malignant) appear as black, breast area gray and background in black.
% show a mammogram and its label

Our database offers adittional information such as age of the patient, breast density, family clinical history, assesment of the subtlety and malignancy of the lesion, etc. This information could be used as complementary features before classification or as labels for the network. In this stage we use only the mammograms with the labels stated above (binary classification).We do not make use of this information.


\subsection{Data division}
70/15/15 is a good split. stratified per patient, this total number of patients in each set and this total number of mammograms (after augmentation) and this total number of lesions.

\begin{table}[h]
	\centering
	\begin{tabular}{ccccc}
		\hline
		&\textbf{Training} & \textbf{Validation} & \textbf{Test} & \textbf{Total}\\
		\hline 
		\textbf{Percentage of data set}	&70	&15	&15	&100\\
		\textbf{Number of patients} &43&10&10&63\\
		\textbf{Number of mammograms} &180&38&38&256\\
		\textbf{Number of masses} &97&21&21&139\\
		\textbf{Mammograms (after augmentation)} &1440&304&38&1782\\
		\hline
	\end{tabular}
	\caption[Data set summary]{Data set summary}
\end{table}

\subsection{Image enhancement}
Background reduction (subtratcint ghe mean intensity of pixels in the breast area) + linear  normalization to change its range to 0-255.
normalization assigns 0 to the minimum gray value in the image and 255 (or the maximum available value) to the maximum gray value in the image and stretches the rest of the values linearly, background reduction plus normalization is similar except that all values below a given threshold (the mean of all pixel values in the image) are mapped to zero and the rest is normalized effectively reducing all small variations in the background to black.

It increases the signal to noise ratio, i.e., highlights lesions over normal breast tissue. On the downside, it also highlights dense structures (which could increase false positives) and may destroy important texture information by blending it with the background. Unpreprocessed images seem too noisy and histogram equalization is too destructive for our purposes.
Also according to Alvarez it seems to help.
% Global contrast normalization makes comparisons across patients not good, because if a pixel is very bright in one patient that is only important with respect to that patient and it may end up (by contrast stretching) having the same value that the pixel in another mammogram who wasn't as bright but was the brightest of the mammogram. i think this makes sense because masss are going ot be bright but only in relation to the patient, maybe one has very fatty tissue so all is white and others have very clear tissue.
% show a figure with the original mammogram and what it wil look like after all this, and its labels.

\subsection{Resizing and cropping}
A mass is usually this size
% Masses are 12.2mm + 4.5mm std . 2 cm holds 96% of masses. 1.5 holds 73 % of masses
Mass sizes (length of the long axis) vary from 5 mm to 20 mm~\cite{Sahiner1996}. Bigger masses are easily detectable in an image and by manual inspection and thus less important for our purposes. 
That's hwta they say and also by visual inspection ins ome triaing examples

To define the size we have to consider two aspects: keeping a manageable input size for the network (in pixels) and capturing the entire lesion in the image patch (in mm).
As we want our network to see the entire mass, we need 112 by 112 pixels of the riginal image to be equivalent to 2 x 2cm. 
There is not really any restriction on spatial resolution other than it being good enough to capture texture information. Again, using an image size of $64 \times 64$ pixels we can cover a 4 $cm^2$ area (2 cm per side) with a spatial resolution of 0.313 mm.
%This dictates the spatial dimensions on our input volume in our convolutional network. The amount of pooling in the architecture defines the stride (and thus the amount of overlapping) during training. the imge produced by the network is of course similar

the saptial resolution is hen reduced by a factor of 2.5. We perform downsamplign with the Lanczos filter offered by Python's PILLOW library. Labels are reduced with a nearest neighbor to make sure it preserves only the tree valid values.
We choose the Lanczos interpolation recommended for downsizing in the PILLOW Python Image Library.



Finally, to avoid having many parts of the image in black, we crop the images to the nearest non-zero pixel in x and y. We use the labels to delimit the breast area and choose a number multiple of 16, so the downsampling by the network is exact. so we recover the (cropped) dimensions by exactly upsampling the produced segmentation by 16.
	
\subsection{Data augmentation}
% 8 usual augmentations
Rotations and flipping of original big images. the breast and cannals, and nipple may be very different at 90 and 270, flipping an 180 seem fine. Or maybe not.
We augment each enhanced image by using 4 rotations (at 0째, 90째, 180째 and 270째) of both the original image and a horizontally flipped version of it, thus we increase our data set by a factor of 8. Both rotations and reflections preserve the original label. In principle it is not neccesary to store the augmented images because they can be easily generated during training but if the disk space is not prohibitive explicitly storing them simplifies training.

Test set was not augmented

% cutting mammograms in 4 or 16 pieces for training woul dnot be that bad, I would have to cut images surrounded with 48 pixels of surrounding regions so the netwrok also sees that and does not see black spaces, then to the output of the network i have to discard a surrounding region of 3 pixels all around to obtain the segmentation of my wanted image: this is exactkly as if training with the big entire image. This if i have no memory, or maybe if i want to change up a bit.

\subsection{Storage} 
Store all augmented images (8 for each mammogram) and their labels (both full version) and downsampled to the output of the network. Store them in the same file from where their original mamogram. 
Format grayscale for images and color for labels
Also for training, store the name of the images. Suffix used for labels versions is tata and for small label is label7.
the entire size of my database is .... with xxx resulting images and corresponding labels (plus the downsampled version). this is shown in table above.
% Be careful with storing as grayscale image, and color labels.
