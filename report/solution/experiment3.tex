To investigate whether negative results are the product of a poorly chosen architecture we designed a different network that is deeper but has fewer parameters than the one used in previous experiments. We use the weighted loss function defined in Section~\ref{subsec:LossFunction2}

\subsection{Architecture}
We model the architecture on the Residual network~\cite{He2015b}, winner of the 2015 ImageNet competition(Tab~\ref{tab:convNetArchitecture3}).

% Arch #28
\begin{table}[h]
	\centering
	\begin{tabular}{lcccccr}
	\hline
	\textbf{Layer} & \textbf{Filter} & \textbf{Stride} & \textbf{Pad} & \textbf{Dilation} & \textbf{Volume} & \textbf{Parameters} \\
	\hline
	\texttt{INPUT}	&- & -	& - & - & $128 \times 128 \times 1$ & -\\
	\texttt{CONV -> LRELU}	& $6 \times 6$ & 2 & 2 & 1 & $64 \times 64 \times 32$ & 1\,184\\
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 1 & 1 & $64 \times 64 \times 32$ & 9\,248\\
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 2 & 1 & 1 & $32 \times 32 \times 64$ & 18\,496\\
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 1 & 1 & $32 \times 32 \times 64$ & 36\,928\\
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 2 & 2 & $32 \times 32 \times 128$ & 73\,856\\
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 2 & 2 & $32 \times 32 \times 128$ & 147\,584\\
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 2 & 2 & $32 \times 32 \times 128$ & 147\,584\\
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 2 & 2 & $32 \times 32 \times 128$ & 147\,584\\
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 4 & 4 & $32 \times 32 \times 256$ & 295\,168\\
	\texttt{CONV}	& $8 \times 8$ & 1 & 14 & 4 & $32 \times 32 \times 1$ & 16\,385\\
	\texttt{BILINEAR (x4)}		& - & - && - & $128 \times 128 \times 1$ & -\\
	\hline
	\end{tabular}
	\caption[Convolutional network architecture for Experiment 3]{Architecture of the network used for experiments. It shows the filter size, stride, dilation and padding in each layer as well as the resulting volume and number of learnable parameters per layer. \texttt{LRELU} stands for leaky ReLU.}
	\label{tab:convNetArchitecture3}
\end{table}

\begin{comment}
\begin{table}[h]
	\centering
	\begin{tabular}{lcccccr}
	\hline
	\textbf{Layer} & \textbf{Filter} & \textbf{Stride} & \textbf{Pad} & \textbf{Dilation} & \textbf{Volume} & \textbf{Parameters} \\ % Receptive field 
	\hline
	\texttt{INPUT}	&- & -	& - & - & $128 \times 128 \times 1$ & -\\
	\texttt{CONV -> LRELU}	& $6 \times 6$ & 2 & 2 & 1 & $64 \times 64 \times 32$ & 1\,184\\ % 6x6
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 1 & 1 & $64 \times 64 \times 32$ & 9\,248\\ % 8x8
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 2 & 2 & $64 \times 64 \times 64$ & 18\,496\\ % 12x12
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 2 & 2 & $64 \times 64 \times 64$ & 36\,928\\ % 16x16
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 4 & 4 & $64 \times 64 \times 128$ & 73\,856\\ % 24x24
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 4 & 4 & $64 \times 64 \times 128$ & 147\,584\\ % 32x32
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 4 & 4 & $64 \times 64 \times 128$ & 147\,584\\ % 40x40
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 4 & 4 & $64 \times 64 \times 128$ & 147\,584\\ % 48x48
	\texttt{CONV -> LRELU}	& $3 \times 3$ & 1 & 8 & 8 & $64 \times 64 \times 256$ & 295\,168\\ % 64x64
	\texttt{CONV}	& $8 \times 8$ & 1 & 28 & 8 & $64 \times 64 \times 1$ & 16\,385\\ % 124x124
	\texttt{BILINEAR (x2)}		& - & - && - & $128 \times 128 \times 1$ & -\\
	\hline
	\end{tabular}
	\caption[Convolutional network architecture for Experiment 3]{Architecture of the network used for experiments. It shows the filter size, stride, dilation and padding in each layer as well as the resulting volume and number of learnable parameters per layer. \texttt{LRELU} stands for leaky ReLU.}
	\label{tab:convNetArchitecture3}
\end{table}
\end{comment}

	 We replace pooling by strided convolutions~\cite{Szegedy2014} or dilated convolutions~\cite{Chen2015}. Input images are whitened independently and passed to the network that produces the predicted segmentation. Because the volume is downsampled only by a factor of four before the upsampling layer, we are able to produce fine-grained segmentations.
	 
	 This 10-layer architecture uses 894\,017 parameters.
	
\subsection{Regularization}
We use l2-norm regularization ($\lambda$ chosen using the validation set) and dropout after RELU layers with probabilities 0.9, 0.9, 0.8, 0.8, 0.7, 0.7, 0.7, 0.7 and 0.6.

\subsection{Hyperparameter search}
We selected $\alpha$ and $\lambda$ using the validation set. We trained 30 networks with sampled $\alpha$ from $10^{unif(-6, -2)}$ and $\lambda$ from $10^{unif(-5, 0)}$; each network was trained for two epochs.
