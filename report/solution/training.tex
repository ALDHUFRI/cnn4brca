We offer details about the hardware and software used for experiments, regularization methods, hyperparameters and implementation notes.

\subsection{Hardware}
Training deep neural networks is computationally intensive and requires equipment with powerful GPUs. We enlist here the resources available for this thesis.
\begin{table}[h]
	\centering
	\begin{tabular}{cp{3.8cm}p{1.7cm}p{1.8cm}cc}
	\hline
	\textbf{PC}	& \textbf{GPU}	& \textbf{HD}	& \textbf{CPU}	& \textbf{RAM}	& \textbf{\#} \\
	\hline
	Personal	& Nvidia NVS 5400M \newline 96 cores, 1GB, compute capability 2.1 & 57 GB \newline (30 free)	& i5-3210M \newline 2.5GHz	& 4 GB	& 1 \\
	A4-401	& Nvidia Quadro K620 \newline 384 cores, 2GB, compute capability 5.0 & 240 GB \newline (230 free)	& i5-4570 \newline 3.2GHz	& 8 GB	& 27\\
	HP-Z400	& AMD Gallium 0.4  & 240 GB \newline (230 free)	& Xeon W3530 \newline 2.8GHz	& 2 GB	& 2\\
	\hline
	\end{tabular}
	\caption{Available hardware for experiments}
\end{table}
We did our experiments in x computers from A3-401

\subsection{Software}
We use Tensorflow to train the networks and Python to develop any other tools (image retrieval and augmentation, model evaluation, figure generation, etc.).
Python3, PILLOW, libraries used,etc.
code available at:

\subsection{Regularization}
Each picture was zero-mean centered, (do not subtract 127)
Dropout and l2-norm, batch normalization


\subsection{Hyperparameters}
Learning rate decay, dropout p, ...
0.1 learning rate divided by 10 when convergence

\subsection{Optimization rule}
ADAM
%Although the gradient rule can be seen as stochastic gradient descen because it uses a single image to compute the gradients it  can also be seen as stochastic radient descent given that it sums the gradients produced by applying the network in different spatial positions.

\subsection{Implementation details}
Upsampling
% If using an upsampling layer, maybe use a simple deconv layer, but change its weights in the forward pass to be the weights needed for bilinear interpolation (or set its weights to always be those for bilinear interpolation), that way gradient descent can take care of it in the backward pass
% For interpolations, see http://paulbourke.net/miscellaneous/interpolation/ y las definiciones de bilinear y bicubic above
Loss function backprop weight mask
% Create the mask when you zero-mean the image (may not work).
Weight initialization
