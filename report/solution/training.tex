We offer details about the hardware and software used for experiments, regularization methods, hyperparameters and implementation notes.

\subsection{Hardware}
Training deep neural networks is computationally intensive and requires equipment with powerful GPUs. We enlist here the resources available for this thesis.
\begin{table}[h]
	\centering
	\begin{tabular}{cp{3.9cm}p{1.7cm}p{1.8cm}cc}
	\hline
	\textbf{PC}	& \textbf{GPU}	& \textbf{HD}	& \textbf{CPU}	& \textbf{RAM}	& \textbf{\#} \\
	\hline
	Personal	& Nvidia NVS 5400M \newline 96 cores, 1GB, 2.1 compatibility, 29 GB/s	& 57 GB \newline (36 free)	& i5-3210M \newline 2.5GHz	& 4 GB	& 1 \\
	A4-401	& Nvidia Quadro K620 \newline 384 cores, 2GB, 5.0 compatibility, 29 GB/s & 240 GB \newline (230 free)	& i5-4570 \newline 3.2GHz	& 8 GB	& 27\\
	\hline
	\end{tabular}
	\caption{Available hardware for experiments}
\end{table}
We did our experiments in x computers from A3-401

\subsection{Software}
We use Tensorflow to train the networks and Python to develop any other tools (image retrieval and augmentation, model evaluation, figure generation, etc.).
Python3, PILLOW, libraries used,etc.
code available at:

\subsection{Regularization}
Each picture was zero-mean centered, (do not subtract 127)
Dropout and l2-norm, batch normalization


\subsection{Hyperparameters}
Learning rate decay, dropout p, ...
0.1 learning rate divided by 10 when convergence

\subsection{Implementation details}
Upsampling
% If using an upsampling layer, maybe use a simple deconv layer, but change its weights in the forward pass to be the weights needed for bilinear interpolation (or set its weights to always be those for bilinear interpolation), that way gradient descent can take care of it in the backward pass
% For interpolations, see http://paulbourke.net/miscellaneous/interpolation/ y las definiciones de bilinear y bicubic above

Loss function backprop
% How do you backpropagate through this mask? Just assign perfect scores to evry position in your predictions that is background (check where is the background in the mask), that way it will be counted by normal gradient, but it will not contribute. I could add a layer at the end (or before the loss function), that sets the gradient to zero in the backwards pass if the position it is viewing is  a background pixel. that'll do it. If yu do judt kill the gradients of thebackground errors, try to report the loss function withouth those values, too, otherwise it will always overestimate the error. Make sure you count over everything that is  not backgorund and not just over everythingthat is 127, otherwise you may not count the lesion
