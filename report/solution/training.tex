We offer details about the learning stage of our experiments.
% including hardware, software and optimization algorithms and other implementation notes.

\subsection{Hardware}
%Training neural networks is computationally intensive and requires equipment with powerful GPUs. 
We performed our experiments in 15 machines with the following specifications:
\begin{table}[h]
	\centering
	\begin{tabular}{cp{3.8cm}p{1.8cm}cc}
	\hline
	\textbf{Location}	& \textbf{GPU}	& \textbf{CPU} &\textbf{HD}	& \textbf{RAM}\\
	\hline
	%Personal	& Nvidia NVS 5400M \newline 96 cores, 1GB, compute capability 2.1 & i5-3210M \newline 2.5GHz & 57 GB & 4 GB\\
	A4-401 & Nvidia Quadro K620 \newline 384 cores, 2GB, compute capability 5.0 & i5-4570 \newline 3.2GHz	& 240 GB & 8 GB\\
	\hline
	\end{tabular}
	\caption{Available hardware for experiments}
\end{table}

Each computer was used independently to train networks, i.e., not distributed. Although we designed our models to be small enough to fit in the GPU memory, big images surpassed this storage limit causing errors; thus, we decided to train our networks only with the CPU. Training times ranged from 1.5 to 2 hours per epoch (1440 images).

\subsection{Software}
Models were implemented and trained using TensorFlow (v.8)~\cite{Abadi2015}. Tools for image retrieval, augmentation, evaluation and similar tasks were implemented in Python3.
Code, as well as trained models, are freely accesible at: \url{github.com/ecobost/cnn4brca}.

\subsection{Initialization}
Weights for the incoming connections to a unit are drawn from a normal distribution with zero mean and $\sqrt{2/n_{in}}$ standard deviation where $n_{in}$ is the number of connections. Biases are initialized to zero.

\subsection{Optimization}
We use ADAM ($\beta_1 = 0.9$, $\beta_2 = 0.995$ and $\epsilon = 10^{-6}$) for optimization.
%, i.e., we perform stochastic gradient descent (with momentum and per-parameter adaptive learning rate).
Each parameter update uses the information from only a single training example but given that the loss is a weighted average over all pixels in the image, gradients are as rich as if we were performing mini-batch gradient descent with a batch composed of all image patches for which the network produced a prediction.

The final model for the first experiment was trained for 50 epochs. The learning rate was divided by 10 after convergence at 30 epochs and then again at 40 epochs. 
% Any early stopping thing here
