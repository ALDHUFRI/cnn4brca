We describe our chosen architecture and loss function.

\subsection{Architecture}
Following recommendations from Section~\ref{sec:PracticalDL}, we define a simple network with six convolutional layers and two fully connected layers (Tab.~\ref{tab:convNetArchitecture}).
\begin{table}[h]
	\centering
	\begin{tabular}{lccccr}
	\hline
	\textbf{Layer} & \textbf{Filter} & \textbf{Stride} &\textbf{Pad} & \textbf{Volume} & \textbf{Parameters} \\
	\hline
	\texttt{INPUT}	& -	& - & - & $112 \times 112 \times 1$ & -\\
	\texttt{CONV -> Leaky RELU} & $6 \times 6$ & 2 & 2 & $56 \times 56 \times 56$ & 2\,072\\
	\texttt{CONV -> Leaky RELU} & $3 \times 3$ & 1 & 1 & $56 \times 56 \times 56$ & 28\,280\\
	\texttt{MAXPOOL} & $2 \times 2$ & 2 & 0 & $28 \times 28 \times 56$ & -\\
	\texttt{CONV -> Leaky RELU} & $3 \times 3$ & 1 & 1 & $28 \times 28 \times 84$ & 42\,420\\
	\texttt{CONV -> Leaky RELU} & $3 \times 3$ & 1 & 1 & $28 \times 28 \times 84$ & 63\,588\\
	\texttt{MAXPOOL} & $2 \times 2$ & 2 & 0 & $14 \times 14 \times 84$ & -\\
	\texttt{CONV -> Leaky RELU} & $3 \times 3$ & 1 & 1 & $14 \times 14 \times 112$ & 84\,784\\
	\texttt{CONV -> Leaky RELU} & $3 \times 3$ & 1 & 1 & $14 \times 14 \times 112$ & 113\,008\\
	\texttt{MAXPOOL} & $2 \times 2$ & 2 & 0 & $7 \times 7 \times 112$ & -\\
	\texttt{FC -> Leaky RELU} & $7 \times 7$ & 1 & 3 & $7 \times 7 \times 448$ & 2\,459\,072\\
	\texttt{FC -> SIGMOID} & $1 \times 1$ & 1 & 0 & $7 \times 7 \times 1$ & 449 \\
	\hline
	\end{tabular}
	\caption[Selected convolutional network architecture]{Architecture of the network used for experiments. It shows the filter size, stride and padding in each layer as well as the resulting volume and number of learnable parameters per layer.}
	\label{tab:convNetArchitecture}
\end{table}


	The first convolutional layer reduces the spatial dimensions of the input from $112 \times 112$ to $56 \times 56$; this reduces the number of parameters and memory requirements of the network. Subsequent convolutional layers preserve the dimensions of its input volume relegating subsampling to pooling layers. 
Produced segmentations are 16 times smaller than the original images (Sec.~\ref{sec:Segmentation}).

This architecture defines 2.79 million parameters. %2 793 673

% Do i need to explain more why this architecture?. Small enough for our data and gpu memory but big enough for the task and to see enough texture, filters are 3x3 as recommended, two convs before pooling.

\subsection{Upsampling}
To simplify the architecture, we downsample our labels to match the produced segmentations rather than adding an upsampling layer at the end of the network.

\subsection{Loss function}
We compute the sigmoid loss function for each pixel in the produced segmentation and sum over all pixels in the breast area, i.e., gradients accumulate for pixels in the breast area. Background is ignored.
% I can weight the losses for different labels differently using the mask (multiplied by the output).
