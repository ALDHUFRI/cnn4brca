We describe our chosen architecture and loss function.

\subsection{Architecture}
%Following recommendations from Section~\ref{sec:PracticalDL}, we define a network with seven convolutional layers and two fully connected layers (Tab.~\ref{tab:convNetArchitecture}). 
We model our architecture on a VGG network~\cite{Simonyan2014}, winner of the 2014 ImageNet competition~(Tab.~\ref{tab:convNetArchitecture}).
\begin{table}[h]
	\centering
	\begin{tabular}{lccccr}
	\hline
	\textbf{Layer} & \textbf{Filter} & \textbf{Stride} &\textbf{Pad} & \textbf{Volume} & \textbf{Parameters} \\
	\hline
	\texttt{INPUT}	& -	& - & - & $112 \times 112 \times 1$ & -\\
	\texttt{CONV -> Leaky RELU} & $6 \times 6$ & 2 & 2 & $56 \times 56 \times 56$ & 2\,072\\
	\texttt{CONV -> Leaky RELU} & $3 \times 3$ & 1 & 1 & $56 \times 56 \times 56$ & 28\,280\\
	\texttt{MAXPOOL} & $2 \times 2$ & 2 & 0 & $28 \times 28 \times 56$ & -\\
	\texttt{CONV -> Leaky RELU} & $3 \times 3$ & 1 & 1 & $28 \times 28 \times 84$ & 42\,420\\
	\texttt{CONV -> Leaky RELU} & $3 \times 3$ & 1 & 1 & $28 \times 28 \times 84$ & 63\,588\\
	\texttt{MAXPOOL} & $2 \times 2$ & 2 & 0 & $14 \times 14 \times 84$ & -\\
	\texttt{CONV -> Leaky RELU} & $3 \times 3$ & 1 & 1 & $14 \times 14 \times 112$ & 84\,784\\
	\texttt{CONV -> Leaky RELU} & $3 \times 3$ & 1 & 1 & $14 \times 14 \times 112$ & 113\,008\\
	\texttt{CONV -> Leaky RELU} & $3 \times 3$ & 1 & 1 & $14 \times 14 \times 112$ & 113\,008\\
	\texttt{MAXPOOL} & $2 \times 2$ & 2 & 0 & $7 \times 7 \times 112$ & -\\
	\texttt{FC -> Leaky RELU} & $7 \times 7$ & 1 & 3 & $7 \times 7 \times 448$ & 2\,459\,072\\
%	\texttt{CONVFC -> Leaky RELU} & $7 \times 7$ & 1 & 3 & $7 \times 7 \times 448$ & 2\,459\,072\\
	\texttt{FC -> SIGMOID} & $1 \times 1$ & 1 & 0 & $7 \times 7 \times 1$ & 449 \\
%	\texttt{CONVFC -> SIGMOID} & $1 \times 1$ & 1 & 0 & $7 \times 7 \times 1$ & 449 \\
%	\texttt{BICUBIC} & - & - & - & $112 \times 112 \times 1$ & -\\
	\hline
	\end{tabular}
	\caption[Selected convolutional network architecture]{Architecture of the network used for experiments. It shows the filter size, stride and padding in each layer as well as the resulting volume and number of learnable parameters per layer.}
	\label{tab:convNetArchitecture}
\end{table}


	The first convolutional layer reduces the spatial dimensions of the input from $112 \times 112$ to $56 \times 56$; this reduces the number of parameters and memory requirements of the network. Subsequent convolutional layers preserve the dimensions of its input volume relegating subsampling to pooling layers. 
Produced segmentations are 16 times smaller than the original images (Sec.~\ref{sec:Segmentation}).

This architecture defines 2.91 million parameters. %2 906 681

% Do i need to explain more why this architecture?. Small enough for our data and gpu memory but big enough for the task and to see enough texture, filters are 3x3 as recommended, two convs before pooling.

% A second architecture modelled on the ResNet~\cite{}, winner of the 2015 ImageNet competition is also used for experiments (Tab~\cite{}).

\subsection{Upsampling}
To simplify the architecture, we downsample our labels to match the produced segmentations rather than adding an upsampling layer at the end of the network.
% Could automatically resize using tf.imageresize_bilinear or bicubic
% change in design decisions and in the layer architecture table above

\subsection{Loss function}
We compute the logistic loss function for each pixel in the produced segmentation and average over all pixels in the breast area, i.e., gradients accumulate over pixels in the breast area (breast tissue and masses) while background is ignored.
%We compute the logistic loss function for each pixel in the produced segmentation and sum over all pixels in the breast area, i.e., gradients accumulate over pixels in the breast area but background is ignored. This amounts to using a weighted loss function where breast area (breast tissue and masses) has weight one and background has weight zero.
% I can weight the losses for breast tissue and breast masses differently using the mask (multiplied by the loss function). To calculate a proper weight, you would have to estimate the prior of a loss being from mass(~0.2) or from area (0.8) and multiply masses by .8 and tissue by 0.2, for instance. Or just decrease the weight of breats tissue, thus masses at 1 and tissue at 0.2 or 0.1 (remember there are images with not a single mass, so mass is quite improbable).
