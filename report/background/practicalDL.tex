In this section we collect guidelines for building as well as efficiently training deep convolutional networks. While they are specific to this thesis, they may prove useful in similar projects. Lastly, deep learning is a fast-changing field so these recommendations may soon be outdated.
% Some of these details are taken care by the software(either as a defualt or optional feature) and some are qyuite new and need to be taken care manually. 

\paragraph{Image preprocessing} Convolutional networks can handle raw image data, but some level of preprocessing speeds training and improves performance.
\begin{itemize}
	\item Crop images to contain only the relevant regions, denoise, enhance and resize them to maintain the input size fixed and manageable.

	\item Zero-center each image feature (the raw pixels) by substracting its mean across all training images. Optionally, normalize each zero-centered feature to range from $[-1 \dots 1]$ by dividing them by its standard deviation~\cite{Karpathy2015}.

	\item Test data should not be used to calculate any statistic used for preprocessing. Furthermore, the same statistics (calculated from training data) should be used to preprocess test data~\cite{Karpathy2015}.
\end{itemize}



\paragraph{Convolutional network architecture} We provide recommendations for designing convolutional networks and sensible values for related hyperparameters.

\begin{itemize}
	\item Select a network architecture flexible enough to model the data and manage overfitting rather than a simpler architecture that may be incapable to model the data~\cite{Ng2014, Krizhevsky2012}. 

	\item Although, theoretically, neural networks with a single hidden layer are universal approximators provided they have enough units ($\mathcal{O}(2^n)$ where $n$ is the size of the input), practically, deeper architectures produce better results using less units overall. This holds for convolutional networks~\cite{Bengio2014}.

	\item Use at least 8 layers (not counting pooling or ReLU layers) for big data sets, use less layers or transfer learning for small data sets. ``You should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.''~\cite{Karpathy2015}

	\item Use 2-3 \texttt{CONV -> RELU} pairs before pooling (N above)~\cite{Karpathy2015}. Pooling is a destructive operation, placing two convolutional layers together allows them to detect more complex features.

	\item Use 1-5 \texttt{[CONV -> RELU]+ -> POOL} blocks (M above). This hyperparameter regulates the representational power of the architecture. The exact number depends on the complexity of the features in the data and the computational resources available. It also defines how much the volume is subsampled.

	\item Use less than 3 \texttt{FC -> RELU} pairs before the output layer (K above)~\cite{Karpathy2015}. The volume that arrives to fully connected layers is already complex, adding many layers only increases the number of parameters and risks overfitting.

	\item The number of feature maps per convolutional layer controls the number of features detected at that layer---similar to the number of units per layer in a regular neural network. A common pattern is to start with a small amount of feature maps and increase them layer by layer~\cite{Simonyan2014}. %The reasoning is that at higher layers there are more complex features to learn and moreover as the feature maps become smaller (via pooling) it is computationally feasible to have more of them.

	\item The number of feature maps per fully connected layer decreases layer by layer\footnote{The number of units in a convolutional layer is the number of units in a feature map times the number of feature maps.}. For instance, for a convolutional network with ten possible classes and two fully connected layers, if the last convolutional layer produces a volume of size $8 \times 8 \times 512$ (8192 units), the first fully connected layer could have size $1 \times 1 \times 2048$ and the second--- the output---layer $1\times 1\times 10$.

	\item Use $3\times 3$ filters with stride 1 and zero-padding 1 or $5 \times 5$ filters with stride 1 and zero-padding 2. This preserves the spatial dimensions of the volume and works better in practice~\cite{Springenberg2014}. If the input size is too big, use a bigger filter in the first convolutional layer~\cite{Karpathy2015}.
	
	\item Use $2\times2$ pooling with stride 2. This pooling and the overlapping version presented in Section~\ref{sec:ConvNets} produce similar results~\cite{Krizhevsky2012}.
%krizhevsky says overlapping is slightly better. so does dieleman but it slows thing down.
	\item Use square input images (width = height) with spatial dimensions divisible by 2. These should be divisible by 2 at least as many times as the number of pooling layers in the network.

	\item Use the number of parameters to measure the complexity of an architecture rather than the number of layers or units.
\end{itemize}



\paragraph{Hyperparameters} About setting and searching hyperparameters other than those of the network architecture.

\begin{itemize}
	\item Use a single sufficiently large validation set (15-30\% of data) rather than cross validation~\cite{Bengio2014}. Use cross validation in very small data sets~\cite{Ng2014}.

	\item Use random search rather than grid search. Random search draws each parameter from a value distribution rather than from a set of predefined values.~\cite{Bergstra2012}

	\item Search for the best combination of hyperparameters rather than each individually.

	\item Train each combination of hyperparameters for 1-2 epochs to narrow the search space; then, train for more epochs on these ranges~\cite{Karpathy2015}. Explore further when the best value for a hyperparameter is found in the limit of the range.~\cite{Bengio2012}.

	\item Partial convergence is sufficient to assess hyperparameters~\cite{Karpathy2015}.	

	%\item Use a different validation set if you need to run the hyperparameter search for new parameters. :: Because the old validation set is already good for the hyperparameters chosen, we want to choose good hyperparameters in general not only in that validation set. 

	\item Hyperparameters related to the convolutional architecture, e.g., number of layers, number of feature maps and filter sizes are set manually (as explained above) rather than using a validation set.

	\item Several hyperparameters are set: initial learning rate $\alpha$, learning rate decay schedule, regularization strength $\lambda$, momentum $\mu$, probability of keeping a unit active in dropout $p$, mini-batch size and type of image preprocessing.

	\item We could fit all hyperparameters using a validation set but, in practice, this is computationally unfeasible and results in overfitting to the validation data~\cite{Cawley2010}.

	\item Set $\alpha$, $\lambda$ and optionally the type of preprocessing using a validation set. Other hyperparameters can be set to a sensible default. 
%The learning rate schedule and training epochs are set using heuristics. 

	\item The learning rate $\alpha$ is ``the single most important hyperparameter and one should always make sure that it has been tuned''~\cite{Bengio2012}. It ranges from $10^{-6}$ to $10^{0}$. Use a log scale to draw new values ($\alpha = 10^{unif(-6, 0)}$ where $unif(a,b)$ is the continous uniform distribution)~\cite{Karpathy2015}.
%0.01: Bengio, 2012 says that the optimal learning rate is close to the highest learning rate that does not cause divergence.

	\item The regularization strength $\lambda$ is usually data (and loss function) dependant. It ranges from $10^{-3}$ to $10^4$. Search in log scale ($\lambda = 10^{unif(-3, 4)}$).
%1
	\item Halve the learning rate every time the validation error stops improving or choose a fixed number of epochs by observing when the validation error stops decreasing in a similar network~\cite{Krizhevsky2012}.

	\item Use $\mu=0.9$. If using a validation set try values in \{0.5, 0.9, 0.95, 0.99\}~\cite{Karpathy2015}.

	\item Use 0.9-1 probability $p$ of retaining a unit in the input layer, 0.65-0.85 in the first 2-4 convolutional layers and 0.5 in the last convolutional layers and all fully connected layers~\cite{Srivastava2014}. Less dropout is used on the first layers because they have less parameters~\cite{Karpathy2015}.
% Maybe don't use dropout in the input layer, because putting a zero there has a meaning(black), maybe the advantage of dropout in cinvolutional layers is just that it adds noise to the input.

	\item Use mini-batch size of 64 or 32. A larger batch size requires more memory and training time. Test performance is unaffected~\cite{Bengio2012}.

	\item Choose among standard preprocessing techniques by (qualitatively) inspecting results on images from the validation set. If none seems superior, fit it along $\alpha$ and $\lambda$% other hyperparameters.
\end{itemize}



\paragraph{Training}
Some general tips for training convolutional networks with millions of parameters and big data sets. Using this advice for small networks may be excessive but it will not hurt the performance.

\begin{itemize}
	\item Randomize the order of the trainig examples before training. As we are using an stochastic estimator of the gradient this ensures the examples in each batch are sampled independently. Shuffling the examples after each epoch could also speed convergence~\cite{Bengio2012}.

	\item To estimate the number of examples needed to train a convolutional network divide the total number of learnable parameters by 25-100 (assuming some data augmentation). Some groups have been able to learn up to 40M parameters from as little as 60K training examples~\cite{Dieleman2015, Springenberg2014}.

	\item Weight initialization is very important for a proper convergence of the network. The current recommendation for ReLU units is to initialize each weight as a value drawn from a gaussian distribution $\mathcal{N}(\mu = 0, \sigma = \sqrt{2/n_{in}})$ where $n_{in}$ is the fan-in of the unit, i.e., the number of inputs to the unit. Specifically, each filter weight could be initialized as \texttt{w = randn()*sqrt(2/nIn)} where \texttt{randn()} returns a value drawn from a standard normal distribution and \texttt{nIn} is the number of connections to this filter (9 for a $3\times 3$ filter, for example). Weights for units in the fully connected layer follow the same formula. Biases can be initialized likewise or to zero~\cite{He2015}.

	\item Use mini-batches to compute the gradient. Using the entire training set to compute the gradient of the loss function takes a big amount of computation and points to the steepest descent direction locally but may not be the right direction if the update step is large. Using mini-batches allows us to make more updates, more frequently which results in faster convergence and better test results~\cite{Bengio2012}.

	\item Use Nesterov's Accelerated Gradient (NAG) to update the weights. It is a modified version of gradient descent which has shown to work slightly better for certain architectures~\cite{Bengio2012b}. Stochastic Gradient Descent with Momentum (SGD+Momentum) is also a viable option~\cite{Karpathy2015}.
% May need to crossvalidate the momentum if using NAG 

	\item Use dropout as a complement to $l_2$-norm regularization. Dropout usually improves results but it may slow network convergence~\cite{Krizhevsky2012}.

	\item Store the network parameters regularly during training. Once per epoch should be enough but it depends on the number of parameters and size of the data. This allows you to come back to different versions of the network and select the one with the best overall validation/test error or one with some special characteristics~\cite{Bengio2014}.

	\item Stop the training process when the validation or test error has not improved since the last learning rate reduction. At this point gradient descent may not have converged but the validation error has and will start to increase (overfit)~\cite{Bengio2012}.

	\item Use the validation or test error to select the best parameters for the network from those stored~\cite{Bengio2014}. 

	\item If you use the test set to refine a model, shuffle the entire data set and choose a diferent training and test set for the new model. Otherwise, you run the risk of overfitting to the test set~\cite{Ng2014}.
\end{itemize}



\paragraph{Sanity checks}
Some simple checks to make sure the training is working properly.
\begin{itemize}
	\item After weight initialization, the network should predict similar scores for each class (uniform probability) and have a loss function (without regularization) equal to $-\log(1/K)$. You can check this by running a test on a small set of examples. Adding regularization should increase the loss~\cite{Karpathy2015}.

	\item If you implement back propagation manually or believe it may not be working properly you can run a gradient check. Gradient checks compare the analytic gradient produced by backpropagation with a numerical gradient produced by a finite difference approximation~\cite{Karpathy2015}.

	\item Train the network with a very small subset of data (20 examples, for instance) and make sure it produces zero loss (without regularization). If it cannot overfit a tiny subset of examples the model is too simple~\cite{Ng2014}.

	\item During training, the training loss should always decrease or only slightly increase. Otherwise, gradient descent may not be working properly either because of an implementation error or poorly tuned hyperparamters (high learning rate, low momentum)~\cite{Karpathy2015}.

	\item Monitor the training and validation loss during training to identify overfitting and underfitting. Underfitting is characterized for a high training loss, overfitting is characterized for a big gap between training and validation (or test) loss~\cite{Ng2014}.
\end{itemize}

\paragraph{Data augmentation}
One of the easiest ways to reduce overfitting in image data is to generate additional examples from the original data by applying some simple label-preserving transformations. Data augmentation allows the network to see different views of the same object thus enabling it to identify features that do not depend on the invariance introduced by the transformations. 
For instance, if we present it with images of a book on different rotations, we expect it to learn to identify a book no matter its position.

\begin{itemize}
	\item There are many transformations one can apply: rotations, translations, horizontal and vertical reflections, crops (sample patches of the original image), zooms, etc. For color images, adding some noise to (jittering) the colors is also a valid transformation.
% Dieleman galaxies didn't like jittering or scales and crops
% Lo liked rotation and horizontal flipping

	\item Exploit the invariances you expect in the data set. For instance, galaxies are rotation invariant given that in space there is no up or down~\cite{Dieleman2015} but trees are not as we rarely see an upside down tree.

	\item When combining different transformations in the same image be careful to preserve the original label. An overly modified image may lose its meaning. 

	\item Most transformations are affine in the geometric plane and can be combined into a single one. If you plan to apply various transformations to the same image, applying a single affine transformation is faster and reduces information loss~\cite{Dieleman2015}.

	\item Generate the augmented images during training. This saves storage and can be performed alongside the training~\cite{Krizhevsky2012}.

	\item Data augmentation can also be used at test time by presenting the network with various versions of the same image and averaging its predictions~\cite{Krizhevsky2012}.
\end{itemize}

\begin{comment}
\paragraph{Image segmentation}
image segmentation
We account for some details of training a convolutional network for image segmentation. 

	\item We usually require to post-process, upsample and threshold the output of a convolutional network. We can fit them using the validation set for no cost given that, at this point, the network is already trained.

\item image postprocessing, using a simple threshold is bad because it doesnt account for multiple comparisons, 

\item CRf work remarkadly well (cite the paper with CRFs) and say that these preprocessing  

%Or I could add this back in hyperparameters
%	\item Use the validation set to select among standard post-processing techniques; this is cheap/efficient because the network does not need to be retrained. It may also be done by visual analysis. some techniques have eeven more paramteres.

	\item Keeo the image sizes manageable with respect to the network because the memory requirements could be excessive. If images are big use small mini-batch sizes.
	\item Use bilinear interpolation. this is usually not important
	\item Set the threshold
	
\end{comment}

% Need some sources for this section. I've read it all around.
\paragraph{Unbalanced data}
Having very few examples of one class compared to the rest is common in practice. We offer here some advice to deal with unbalanced classes using standard convolutional networks. We note that there is no accepted way to manage this problem.
\begin{itemize}
	\item For a binary classifier, if the positive class is the rare class use PRAUC as a performance metric. If you are reporting the $F_1$ score, you should select an appropiate threshold using a validation set~\cite{Davis2006}.

	\item If using PRAUC or selecting the threshold with a validation set is impractical, a simple adjustment is to divide the predicted probabilities by their corresponding class priors and to renormalize the values.

	\item For multiclass classification, use the macro-averaged $F_1$ score, an average of $F_1$ scores per class, with validated thresholds~\cite{Ozgur2005}. A multiclass PRAUC also exists but it is not as easy to interpret.

	\item As the threshold setting does not affect training it can be fitted independently of other hyperparameters once the network has already been trained. If fitting more than one, consider each one as a separate hyperparameter and use random search to find the best combination.

	\item One of the preferred methods to learn with unbalanced data sets is to use a modified loss function which gives a higher weight to errors in the rare class so that during training errors in the rare class will produce higher learning in the network parameters. Specific knowledge of the domain is required to estimate the cost of each class of error.

	\item Oversampling and undersampling, repeating the examples of the rare class or discarding some examples from the dominant class, are discouraged because they either not add information or throw away some of it.

	\item Replicating rare examples (oversampling) is useful when the examples are very scarce and the classifier simply does not have enough data to learn. This could be achieved by balancing the classes on each mini-batch via stratified sampling or by augmenting the rare class more than the dominant class during data augmentation.

	\item Data augmentation differs from data replication in that it only tries to enrich the data set with invariant images but actually leaves the proportion of classes unchanged.
\end{itemize}

\begin{comment}
\paragraph{All convolutional networks}
The research community has been moving towards discarding the pooling layers and using all convolutional networks. This can be interpreted as letting the network learn the pooling operation. We offer a couple of guidelines for implementing all convolutional networks.  
\begin{itemize}
	\item Replace each pooling layer with a convolutional layer with as many feature maps as the previous layer and filter size $2\times 2$ with stride $2$ for normal pooling or $3\times 3$ with stride $2$ for overlapping pooling~\cite{Springenberg2014}. 

	\item For small data sets pooling layers also work as a regularizer because they reduce the number of learnable parameters and replacing them with convolutional layers may not be convenient~\cite{Karpathy2015}.
\end{itemize}

\paragraph{Transfer learning}
When we have a small data set we could use a pretrained convolutional network either as a feature extractor for the new examples or to provide initializations for the new convolutional network, this is called transfer learning. We offer some tips for using a pretrained model specifically for mammographic images.

\begin{itemize}
	\item Using a convolutional network pretrained in natural images, such as the ImageNet database, CIFAR-10, CIFAR-100, etc., may not work for mammographic images because features useful for one kind of classification are not very useful for the other. Nonetheless, given that features become more specific at higher layers, we could discard the higher layers of the network and use only the cropped network~\cite{Karpathy2015}.
 
	\item Depending on the amount of data that we have we could: (1) add some fully connected layers on top of the pretrained network and train only these new layers, (2) add some convoutional and fully connected layers and train these new layers or (3) add convolutional and/or fully connected layers and train the entire network~\cite{Karpathy2015}.

	\item When training on a pretrained model or fine-tuning use an smaller learning rate than when training a network from scratch. Using a small learning rate assures that we do not disturb very much the already good network parameters.~\cite{Karpathy2015}.
\end{itemize}
\end{comment}

\paragraph{Software}A short description of four of the most popular packages for deep learning. They are pretty similar in capabilities and availability (open-source).

\begin{itemize}
	\item Tensorflow~\cite{Abadi2015} is a Python/C++ library released by Google that supports automatic differention on data flow graphs---neural networks, included, training on clusters of CPUs and GPUs, and easy deployment in multiple platforms. It has been quickly adopted by the deep learning community.
	\item Caffe~\cite{Jia2014}: Caffe is an already mature deep learning framework developed in C++/CUDA by the Berkeley Vision and Learning Center (BVLC) and community contributors. It offers a command line, Python and Matlab interface, reference models and tutorials and very fast code with easy GPU activation.
	\item Theano~\cite{Bergstra2010, Bastien2012}: Theano is a Python library developed in Python/CUDA at the University of Montreal. It is tightly integrated with NumPy, performs symbolic automatic differentiation and uses the GPU to efficiently evaluate mathematical expressions involving multi dimensional arrays.
	\item Torch7~\cite{Collobert2011}: Torch is a scientific computing framework developed in C/Lua/CUDA at the IDIAP Research Institute. It offers n-dimensional arrays (tensors), automatic differentiation, a command line and Lua interface, GPU support and easy building of complex neural network architectures.
%	\item Cuda-ConvNet2~\cite{Krizhevsky2014}: Cuda-ConvNet2 is a highly optimized convolutional network library developed in C++/CUDA by Alex Krizhevsky. It offers different off-the-shelf configurations for convolutional networks, a command line interface and multi-GPU training.
\end{itemize}

\bigskip

We acknowledge that mammographic data is different from common image segmentation data: labelling is imperfect, image sizes and ratio change, images are bigger, quality varies, objects of interest are small in relation to the background, data sets are smaller, texture is uniform across the image, among others. Therefore, some of the advice given above may prove counterproductive. If possible, design decisions should be based on data and results.
%Deep Learning (Bengio Lecture): (https://www.youtube.com/watch?v=JuimBuvEWBg)
